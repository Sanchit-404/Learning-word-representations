{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b8fb671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset phrase_similarity/PS-hard to C:/Users/sanch/.cache/huggingface/datasets/PiC___phrase_similarity/PS-hard/2.0.1/7edf99614bd0c5125d3fc3b7d592122a988f23efc1e72dd1e01759bc68359302...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3215f4a6bbf4d8aa0a93fe40ae75c4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a970538fdbf14a34887861e86797785a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4179cebdf76345f9926f213d676cbe41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/457k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c152e2c4664ed1a6312ece08f7e8ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/917k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e65eff53e27b4bf5872aaa01b77a8fa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5f5a1c4953d4e8f956a383fd0691110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519825411dba45caab54bf5ec94c6e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset phrase_similarity downloaded and prepared to C:/Users/sanch/.cache/huggingface/datasets/PiC___phrase_similarity/PS-hard/2.0.1/7edf99614bd0c5125d3fc3b7d592122a988f23efc1e72dd1e01759bc68359302. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d8e16a5ad2a4d0f82628c0ed8a281de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              sentence1  \\\n",
      "0     newly formed camp is released from the membran...   \n",
      "1     According to one data, in 1910, on others â€“ in...   \n",
      "2     Note that Fact 1 does not assume any particula...   \n",
      "3     Assessment-Center are usually group-processes ...   \n",
      "4     At the end of the 1980s, a different cross had...   \n",
      "...                                                 ...   \n",
      "6999  Nereus and Achilleus a similar notice was admi...   \n",
      "7000  Color quantization is the process of creating ...   \n",
      "7001  petra is a dutch tool with which patients and ...   \n",
      "7002  However, granting this access, especially to a...   \n",
      "7003  The show was immediately picked up for a full ...   \n",
      "\n",
      "                                              sentence2  cosine_similarity  \n",
      "0     recently made encampment is released from the ...           0.999982  \n",
      "1     According to a particular statistic, in 1910, ...           0.999819  \n",
      "2     Note that Fact 1 does not assume any specific ...           0.999979  \n",
      "3     Assessment-Center are usually group-processes ...           0.999984  \n",
      "4     At the end of the 1980s, a opposing inquiries ...           0.999940  \n",
      "...                                                 ...                ...  \n",
      "6999  Nereus and Achilleus a comparable notification...           0.999980  \n",
      "7000  Color quantization is the process of creating ...           0.999998  \n",
      "7001  petra is a device from the people of the Nethe...           0.997012  \n",
      "7002  However, granting this access, especially to a...           0.999996  \n",
      "7003  The show was immediately picked up for a compr...           0.999960  \n",
      "\n",
      "[7004 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "\n",
    "pic_dataset = load_dataset('PiC/phrase_similarity', 'PS-hard')\n",
    "pic_df = pd.DataFrame(pic_dataset['train'])\n",
    "\n",
    "corpus = pic_df['sentence1'].tolist() + pic_df['sentence2'].tolist()\n",
    "word2vec_model = Word2Vec(sentences=[sentence.split() for sentence in corpus], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "def get_sentence_vector(sentence, model):\n",
    "    words = sentence.split()\n",
    "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if vectors:\n",
    "        return sum(vectors) / len(vectors)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "pic_df['vector_sentence1'] = pic_df['sentence1'].apply(lambda x: get_sentence_vector(x, word2vec_model))\n",
    "pic_df['vector_sentence2'] = pic_df['sentence2'].apply(lambda x: get_sentence_vector(x, word2vec_model))\n",
    "\n",
    "pic_df = pic_df.dropna(subset=['vector_sentence1', 'vector_sentence2'])\n",
    "pic_df['cosine_similarity'] = pic_df.apply(lambda row: cosine_similarity([row['vector_sentence1']], [row['vector_sentence2']])[0][0], axis=1)\n",
    "print(pic_df[['sentence1', 'sentence2', 'cosine_similarity']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "832c3682",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset phrase_similarity (C:/Users/sanch/.cache/huggingface/datasets/PiC___phrase_similarity/PS-hard/2.0.1/7edf99614bd0c5125d3fc3b7d592122a988f23efc1e72dd1e01759bc68359302)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9216cc6ba79240f1b7f9eab330ff2d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4989293361884368\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "pic_dataset = load_dataset('PiC/phrase_similarity', 'PS-hard')\n",
    "\n",
    "pic_df = pd.DataFrame(pic_dataset['train'])\n",
    "\n",
    "corpus = pic_df['sentence1'].tolist() + pic_df['sentence2'].tolist()\n",
    "word2vec_model = Word2Vec(sentences=[sentence.split() for sentence in corpus], vector_size=100, window=5, min_count=1, workers=4)\n",
    "def get_sentence_vector(sentence, model):\n",
    "    words = sentence.split()\n",
    "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if vectors:\n",
    "        return sum(vectors) / len(vectors)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "pic_df['vector_sentence1'] = pic_df['sentence1'].apply(lambda x: get_sentence_vector(x, word2vec_model))\n",
    "pic_df['vector_sentence2'] = pic_df['sentence2'].apply(lambda x: get_sentence_vector(x, word2vec_model))\n",
    "pic_df = pic_df.dropna(subset=['vector_sentence1', 'vector_sentence2'])\n",
    "\n",
    "pic_df['cosine_similarity'] = pic_df.apply(lambda row: cosine_similarity([row['vector_sentence1']], [row['vector_sentence2']])[0][0], axis=1)\n",
    "\n",
    "X = pic_df[['cosine_similarity']]\n",
    "y = pic_df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72b9c513",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset phrase_similarity (C:/Users/sanch/.cache/huggingface/datasets/PiC___phrase_similarity/PS-hard/2.0.1/7edf99614bd0c5125d3fc3b7d592122a988f23efc1e72dd1e01759bc68359302)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7859673b7f43be958a81990a59cd80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "141/141 [==============================] - 29s 61ms/step - loss: 0.6932 - accuracy: 0.4955 - val_loss: 0.6932 - val_accuracy: 0.4906\n",
      "Epoch 2/10\n",
      "141/141 [==============================] - 6s 44ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.4906\n",
      "Epoch 3/10\n",
      "141/141 [==============================] - 6s 42ms/step - loss: 0.6932 - accuracy: 0.4993 - val_loss: 0.6932 - val_accuracy: 0.4906\n",
      "Epoch 4/10\n",
      "141/141 [==============================] - 6s 45ms/step - loss: 0.6932 - accuracy: 0.5022 - val_loss: 0.6933 - val_accuracy: 0.4906\n",
      "Epoch 5/10\n",
      "141/141 [==============================] - 6s 45ms/step - loss: 0.6932 - accuracy: 0.5025 - val_loss: 0.6932 - val_accuracy: 0.4906\n",
      "Epoch 6/10\n",
      "141/141 [==============================] - 6s 42ms/step - loss: 0.6932 - accuracy: 0.5054 - val_loss: 0.6932 - val_accuracy: 0.4906\n",
      "Epoch 7/10\n",
      "141/141 [==============================] - 6s 40ms/step - loss: 0.6932 - accuracy: 0.4922 - val_loss: 0.6933 - val_accuracy: 0.4915\n",
      "Epoch 8/10\n",
      "141/141 [==============================] - 6s 44ms/step - loss: 0.6932 - accuracy: 0.5027 - val_loss: 0.6933 - val_accuracy: 0.4906\n",
      "Epoch 9/10\n",
      "141/141 [==============================] - 7s 46ms/step - loss: 0.6931 - accuracy: 0.5038 - val_loss: 0.6932 - val_accuracy: 0.4906\n",
      "Epoch 10/10\n",
      "141/141 [==============================] - 6s 45ms/step - loss: 0.6932 - accuracy: 0.5054 - val_loss: 0.6933 - val_accuracy: 0.4906\n",
      "44/44 [==============================] - 3s 16ms/step\n",
      "Siamese Model Accuracy on Test Set: 0.48394004282655245\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "pic_dataset = load_dataset('PiC/phrase_similarity', 'PS-hard')\n",
    "\n",
    "pic_df = pd.DataFrame(pic_dataset['train'])\n",
    "label_encoder = LabelEncoder()\n",
    "pic_df['label'] = label_encoder.fit_transform(pic_df['label'])\n",
    "train_df, test_df = train_test_split(pic_df, test_size=0.2, random_state=42)\n",
    "corpus = pic_df['sentence1'].tolist() + pic_df['sentence2'].tolist()\n",
    "word2vec_model = Word2Vec(sentences=[sentence.split() for sentence in corpus], vector_size=100, window=5, min_count=1, workers=4)\n",
    "def get_sentence_vector(sentence, model):\n",
    "    words = sentence.split()\n",
    "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if vectors:\n",
    "        return sum(vectors) / len(vectors)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "train_df['vector_sentence1'] = train_df['sentence1'].apply(lambda x: get_sentence_vector(x, word2vec_model))\n",
    "train_df['vector_sentence2'] = train_df['sentence2'].apply(lambda x: get_sentence_vector(x, word2vec_model))\n",
    "\n",
    "train_df = train_df.dropna(subset=['vector_sentence1', 'vector_sentence2'])\n",
    "\n",
    "pairs = []\n",
    "labels = []\n",
    "for _, row in train_df.iterrows():\n",
    "    pairs.append([row['vector_sentence1'], row['vector_sentence2']])\n",
    "    labels.append(row['label'])\n",
    "\n",
    "pairs = np.array(pairs)\n",
    "labels = np.array(labels)\n",
    "\n",
    "input_shape = (100,)  \n",
    "\n",
    "input_1 = Input(shape=input_shape)\n",
    "input_2 = Input(shape=input_shape)\n",
    "\n",
    "embedding_layer = Embedding(input_dim=1, output_dim=100, input_length=input_shape[0])\n",
    "lstm_layer = LSTM(64)\n",
    "\n",
    "encoded_1 = lstm_layer(embedding_layer(input_1))\n",
    "encoded_2 = lstm_layer(embedding_layer(input_2))\n",
    "\n",
    "distance = Lambda(lambda x: K.abs(x[0] - x[1]))([encoded_1, encoded_2])\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(distance)\n",
    "\n",
    "siamese_model = Model(inputs=[input_1, input_2], outputs=output)\n",
    "\n",
    "siamese_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "siamese_model.fit([pairs[:, 0], pairs[:, 1]], labels, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "test_pairs = []\n",
    "test_labels = []\n",
    "\n",
    "for _, row in test_df.iterrows():\n",
    "    vector1 = get_sentence_vector(row['sentence1'], word2vec_model)\n",
    "    vector2 = get_sentence_vector(row['sentence2'], word2vec_model)\n",
    "\n",
    "    if vector1 is not None and vector2 is not None:\n",
    "        test_pairs.append([vector1, vector2])\n",
    "        test_labels.append(row['label'])\n",
    "\n",
    "test_pairs = np.array(test_pairs)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "predictions = siamese_model.predict([test_pairs[:, 0], test_pairs[:, 1]])\n",
    "\n",
    "binary_predictions = (predictions > 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(test_labels, binary_predictions)\n",
    "print(f'Siamese Model Accuracy on Test Set: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "293cc888",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset phrase_similarity (C:/Users/sanch/.cache/huggingface/datasets/PiC___phrase_similarity/PS-hard/2.0.1/7edf99614bd0c5125d3fc3b7d592122a988f23efc1e72dd1e01759bc68359302)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72aabdaa95a24f828a4c33e4376c30e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sanch\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py:1699: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  return t[start:end]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - 8s 22ms/step - loss: 0.6956 - accuracy: 0.5056 - val_loss: 0.6930 - val_accuracy: 0.5085\n",
      "Epoch 2/10\n",
      "141/141 [==============================] - 2s 12ms/step - loss: 0.6948 - accuracy: 0.4888 - val_loss: 0.6939 - val_accuracy: 0.4906\n",
      "Epoch 3/10\n",
      "141/141 [==============================] - 2s 13ms/step - loss: 0.6949 - accuracy: 0.4913 - val_loss: 0.6936 - val_accuracy: 0.4942\n",
      "Epoch 4/10\n",
      "141/141 [==============================] - 2s 12ms/step - loss: 0.6937 - accuracy: 0.5016 - val_loss: 0.6944 - val_accuracy: 0.4915\n",
      "Epoch 5/10\n",
      "141/141 [==============================] - 2s 12ms/step - loss: 0.6937 - accuracy: 0.4955 - val_loss: 0.6930 - val_accuracy: 0.5040\n",
      "Epoch 6/10\n",
      "141/141 [==============================] - 2s 11ms/step - loss: 0.6934 - accuracy: 0.4897 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 7/10\n",
      "141/141 [==============================] - 2s 11ms/step - loss: 0.6930 - accuracy: 0.5120 - val_loss: 0.6934 - val_accuracy: 0.5085\n",
      "Epoch 8/10\n",
      "141/141 [==============================] - 2s 12ms/step - loss: 0.6938 - accuracy: 0.4917 - val_loss: 0.6936 - val_accuracy: 0.4924\n",
      "Epoch 9/10\n",
      "141/141 [==============================] - 2s 12ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6937 - val_accuracy: 0.4915\n",
      "Epoch 10/10\n",
      "141/141 [==============================] - 2s 13ms/step - loss: 0.6933 - accuracy: 0.5045 - val_loss: 0.6932 - val_accuracy: 0.5067\n",
      "44/44 [==============================] - 1s 4ms/step\n",
      "Accuracy: 0.48679514632405424\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from gensim.models import Word2Vec\n",
    "from datasets import load_dataset\n",
    "\n",
    "pic_dataset = load_dataset('PiC/phrase_similarity', 'PS-hard')\n",
    "\n",
    "pic_df = pd.DataFrame(pic_dataset['train'])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "pic_df['label'] = label_encoder.fit_transform(pic_df['label'])\n",
    "\n",
    "corpus = pic_df['sentence1'].tolist() + pic_df['sentence2'].tolist()\n",
    "word2vec_model = Word2Vec(sentences=[sentence.split() for sentence in corpus], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "def get_sentence_vector(sentence, model):\n",
    "    words = sentence.split()\n",
    "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if vectors:\n",
    "        return sum(vectors) / len(vectors)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "pic_df['vector_sentence1'] = pic_df['sentence1'].apply(lambda x: get_sentence_vector(x, word2vec_model))\n",
    "pic_df['vector_sentence2'] = pic_df['sentence2'].apply(lambda x: get_sentence_vector(x, word2vec_model))\n",
    "\n",
    "pic_df = pic_df.dropna(subset=['vector_sentence1', 'vector_sentence2'])\n",
    "\n",
    "X = np.array(list(zip(pic_df['vector_sentence1'], pic_df['vector_sentence2'])))\n",
    "y = pic_df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(2, 100))) \n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "526ed092",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset phrase_similarity (C:/Users/sanch/.cache/huggingface/datasets/PiC___phrase_similarity/PS-hard/2.0.1/7edf99614bd0c5125d3fc3b7d592122a988f23efc1e72dd1e01759bc68359302)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6c2d89d3494448b933a1450cfeb125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sanch\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py:1699: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  return t[start:end]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - 5s 16ms/step - loss: 0.7005 - accuracy: 0.4940 - val_loss: 0.6939 - val_accuracy: 0.5085\n",
      "Epoch 2/10\n",
      "141/141 [==============================] - 2s 11ms/step - loss: 0.6955 - accuracy: 0.5042 - val_loss: 0.6939 - val_accuracy: 0.5085\n",
      "Epoch 3/10\n",
      "141/141 [==============================] - 2s 13ms/step - loss: 0.6962 - accuracy: 0.5013 - val_loss: 0.6946 - val_accuracy: 0.4915\n",
      "Epoch 4/10\n",
      "141/141 [==============================] - 1s 11ms/step - loss: 0.6938 - accuracy: 0.5011 - val_loss: 0.6970 - val_accuracy: 0.4915\n",
      "Epoch 5/10\n",
      "141/141 [==============================] - 2s 11ms/step - loss: 0.6950 - accuracy: 0.4933 - val_loss: 0.6979 - val_accuracy: 0.4915\n",
      "Epoch 6/10\n",
      "141/141 [==============================] - 2s 11ms/step - loss: 0.6942 - accuracy: 0.4926 - val_loss: 0.6933 - val_accuracy: 0.4942\n",
      "Epoch 7/10\n",
      "141/141 [==============================] - 2s 11ms/step - loss: 0.6942 - accuracy: 0.4955 - val_loss: 0.6942 - val_accuracy: 0.4897\n",
      "Epoch 8/10\n",
      "141/141 [==============================] - 2s 11ms/step - loss: 0.6950 - accuracy: 0.5000 - val_loss: 0.6935 - val_accuracy: 0.4880\n",
      "Epoch 9/10\n",
      "141/141 [==============================] - 2s 12ms/step - loss: 0.6937 - accuracy: 0.4989 - val_loss: 0.6940 - val_accuracy: 0.4880\n",
      "Epoch 10/10\n",
      "141/141 [==============================] - 1s 11ms/step - loss: 0.6936 - accuracy: 0.4891 - val_loss: 0.6942 - val_accuracy: 0.4924\n",
      "44/44 [==============================] - 1s 3ms/step\n",
      "Accuracy: 0.49964311206281226\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from gensim.models import Word2Vec\n",
    "from datasets import load_dataset\n",
    "\n",
    "pic_dataset = load_dataset('PiC/phrase_similarity', 'PS-hard')\n",
    "\n",
    "pic_df = pd.DataFrame(pic_dataset['train'])\n",
    "label_encoder = LabelEncoder()\n",
    "pic_df['label'] = label_encoder.fit_transform(pic_df['label'])\n",
    "\n",
    "corpus = pic_df['sentence1'].tolist() + pic_df['sentence2'].tolist()\n",
    "word2vec_model = Word2Vec(sentences=[sentence.split() for sentence in corpus], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "def get_sentence_vector(sentence, model):\n",
    "    words = sentence.split()\n",
    "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if vectors:\n",
    "        return sum(vectors) / len(vectors)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "pic_df['vector_sentence1'] = pic_df['sentence1'].apply(lambda x: get_sentence_vector(x, word2vec_model))\n",
    "pic_df['vector_sentence2'] = pic_df['sentence2'].apply(lambda x: get_sentence_vector(x, word2vec_model))\n",
    "\n",
    "pic_df = pic_df.dropna(subset=['vector_sentence1', 'vector_sentence2'])\n",
    "\n",
    "X = np.array(list(zip(pic_df['vector_sentence1'], pic_df['vector_sentence2'])))\n",
    "y = pic_df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(64, input_shape=(2, 100), activation='relu'))  # Adjust input_shape based on your sentence vector dimensions\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec114ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset phrase_similarity (C:/Users/sanch/.cache/huggingface/datasets/PiC___phrase_similarity/PS-hard/2.0.1/7edf99614bd0c5125d3fc3b7d592122a988f23efc1e72dd1e01759bc68359302)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b314d6349e5e4100acec2468b3537424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sanch\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py:1699: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  return t[start:end]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - 5s 16ms/step - loss: 0.6995 - accuracy: 0.4949 - val_loss: 0.6950 - val_accuracy: 0.4915\n",
      "Epoch 2/10\n",
      "141/141 [==============================] - 2s 11ms/step - loss: 0.6964 - accuracy: 0.4857 - val_loss: 0.6946 - val_accuracy: 0.5085\n",
      "Epoch 3/10\n",
      "141/141 [==============================] - 2s 11ms/step - loss: 0.6959 - accuracy: 0.4996 - val_loss: 0.6941 - val_accuracy: 0.4915\n",
      "Epoch 4/10\n",
      "141/141 [==============================] - 1s 11ms/step - loss: 0.6957 - accuracy: 0.4924 - val_loss: 0.6978 - val_accuracy: 0.4915\n",
      "Epoch 5/10\n",
      "141/141 [==============================] - 1s 10ms/step - loss: 0.6968 - accuracy: 0.4893 - val_loss: 0.6950 - val_accuracy: 0.4915\n",
      "Epoch 6/10\n",
      "141/141 [==============================] - 1s 10ms/step - loss: 0.6943 - accuracy: 0.5011 - val_loss: 0.6937 - val_accuracy: 0.4915\n",
      "Epoch 7/10\n",
      "141/141 [==============================] - 1s 11ms/step - loss: 0.6939 - accuracy: 0.5011 - val_loss: 0.6933 - val_accuracy: 0.4924\n",
      "Epoch 8/10\n",
      "141/141 [==============================] - 2s 11ms/step - loss: 0.6938 - accuracy: 0.5049 - val_loss: 0.6931 - val_accuracy: 0.5085\n",
      "Epoch 9/10\n",
      "141/141 [==============================] - 2s 12ms/step - loss: 0.6941 - accuracy: 0.4895 - val_loss: 0.6944 - val_accuracy: 0.4915\n",
      "Epoch 10/10\n",
      "141/141 [==============================] - 2s 11ms/step - loss: 0.6936 - accuracy: 0.5002 - val_loss: 0.6954 - val_accuracy: 0.4915\n",
      "44/44 [==============================] - 1s 3ms/step\n",
      "Accuracy: 0.4989293361884368\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from gensim.models import FastText\n",
    "from datasets import load_dataset\n",
    "\n",
    "pic_dataset = load_dataset('PiC/phrase_similarity', 'PS-hard')\n",
    "\n",
    "pic_df = pd.DataFrame(pic_dataset['train'])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "pic_df['label'] = label_encoder.fit_transform(pic_df['label'])\n",
    "\n",
    "corpus = pic_df['sentence1'].tolist() + pic_df['sentence2'].tolist()\n",
    "fasttext_model = FastText(sentences=[sentence.split() for sentence in corpus], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "def get_sentence_vector(sentence, model):\n",
    "    words = sentence.split()\n",
    "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if vectors:\n",
    "        return sum(vectors) / len(vectors)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "pic_df['vector_sentence1'] = pic_df['sentence1'].apply(lambda x: get_sentence_vector(x, fasttext_model))\n",
    "pic_df['vector_sentence2'] = pic_df['sentence2'].apply(lambda x: get_sentence_vector(x, fasttext_model))\n",
    "\n",
    "pic_df = pic_df.dropna(subset=['vector_sentence1', 'vector_sentence2'])\n",
    "\n",
    "X = np.array(list(zip(pic_df['vector_sentence1'], pic_df['vector_sentence2'])))\n",
    "y = pic_df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(64, input_shape=(2, 100), activation='relu'))  # Adjust input_shape based on your sentence vector dimensions\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ab9ed7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset phrase_similarity (C:/Users/sanch/.cache/huggingface/datasets/PiC___phrase_similarity/PS-hard/2.0.1/7edf99614bd0c5125d3fc3b7d592122a988f23efc1e72dd1e01759bc68359302)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26e53258874b4f7d9638c0273066760e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sanch\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py:1699: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  return t[start:end]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - 5s 18ms/step - loss: 0.6967 - accuracy: 0.4996 - val_loss: 0.7021 - val_accuracy: 0.4915\n",
      "Epoch 2/10\n",
      "141/141 [==============================] - 1s 10ms/step - loss: 0.6958 - accuracy: 0.4906 - val_loss: 0.6930 - val_accuracy: 0.5076\n",
      "Epoch 3/10\n",
      "141/141 [==============================] - 1s 11ms/step - loss: 0.6943 - accuracy: 0.5042 - val_loss: 0.6970 - val_accuracy: 0.4915\n",
      "Epoch 4/10\n",
      "141/141 [==============================] - 1s 10ms/step - loss: 0.6938 - accuracy: 0.4913 - val_loss: 0.6935 - val_accuracy: 0.4915\n",
      "Epoch 5/10\n",
      "141/141 [==============================] - 2s 11ms/step - loss: 0.6937 - accuracy: 0.4949 - val_loss: 0.6931 - val_accuracy: 0.5085\n",
      "Epoch 6/10\n",
      "141/141 [==============================] - 1s 11ms/step - loss: 0.6942 - accuracy: 0.4920 - val_loss: 0.6934 - val_accuracy: 0.4915\n",
      "Epoch 7/10\n",
      "141/141 [==============================] - 2s 11ms/step - loss: 0.6940 - accuracy: 0.4929 - val_loss: 0.6932 - val_accuracy: 0.4880\n",
      "Epoch 8/10\n",
      "141/141 [==============================] - 2s 11ms/step - loss: 0.6937 - accuracy: 0.5011 - val_loss: 0.6931 - val_accuracy: 0.5085\n",
      "Epoch 9/10\n",
      "141/141 [==============================] - 1s 10ms/step - loss: 0.6934 - accuracy: 0.4967 - val_loss: 0.6931 - val_accuracy: 0.5058\n",
      "Epoch 10/10\n",
      "141/141 [==============================] - 2s 11ms/step - loss: 0.6940 - accuracy: 0.4935 - val_loss: 0.6932 - val_accuracy: 0.4871\n",
      "44/44 [==============================] - 1s 3ms/step\n",
      "Accuracy: 0.4982155603140614\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from gensim.models import Word2Vec\n",
    "from datasets import load_dataset\n",
    "\n",
    "pic_dataset = load_dataset('PiC/phrase_similarity', 'PS-hard')\n",
    "\n",
    "pic_df = pd.DataFrame(pic_dataset['train'])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "pic_df['label'] = label_encoder.fit_transform(pic_df['label'])\n",
    "\n",
    "corpus = pic_df['sentence1'].tolist() + pic_df['sentence2'].tolist()\n",
    "corpus = pic_df['sentence1'].tolist() + pic_df['sentence2'].tolist()\n",
    "fasttext_model = FastText(sentences=[sentence.split() for sentence in corpus], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "def get_sentence_vector(sentence, model):\n",
    "    words = sentence.split()\n",
    "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if vectors:\n",
    "        return sum(vectors) / len(vectors)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "pic_df['vector_sentence1'] = pic_df['sentence1'].apply(lambda x: get_sentence_vector(x, fasttext_model))\n",
    "pic_df['vector_sentence2'] = pic_df['sentence2'].apply(lambda x: get_sentence_vector(x, fasttext_model))\n",
    "\n",
    "pic_df = pic_df.dropna(subset=['vector_sentence1', 'vector_sentence2'])\n",
    "\n",
    "X = np.array(list(zip(pic_df['vector_sentence1'], pic_df['vector_sentence2'])))\n",
    "y = pic_df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(64, input_shape=(2, 100), activation='relu'))  # Adjust input_shape based on your sentence vector dimensions\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "452c24bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset phrase_similarity (C:/Users/sanch/.cache/huggingface/datasets/PiC___phrase_similarity/PS-hard/2.0.1/7edf99614bd0c5125d3fc3b7d592122a988f23efc1e72dd1e01759bc68359302)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3bbc51fa1424758b03b0151b1de7043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sanch\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py:1699: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  return t[start:end]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - 8s 20ms/step - loss: 0.6961 - accuracy: 0.5002 - val_loss: 0.6959 - val_accuracy: 0.5085\n",
      "Epoch 2/20\n",
      "141/141 [==============================] - 2s 12ms/step - loss: 0.6950 - accuracy: 0.4851 - val_loss: 0.6942 - val_accuracy: 0.5085\n",
      "Epoch 3/20\n",
      "141/141 [==============================] - 2s 11ms/step - loss: 0.6945 - accuracy: 0.4866 - val_loss: 0.6934 - val_accuracy: 0.4871\n",
      "Epoch 4/20\n",
      "141/141 [==============================] - 2s 11ms/step - loss: 0.6940 - accuracy: 0.4998 - val_loss: 0.6931 - val_accuracy: 0.4897\n",
      "Epoch 5/20\n",
      "141/141 [==============================] - 2s 11ms/step - loss: 0.6935 - accuracy: 0.5011 - val_loss: 0.6946 - val_accuracy: 0.4915\n",
      "Epoch 6/20\n",
      "141/141 [==============================] - 2s 11ms/step - loss: 0.6938 - accuracy: 0.4980 - val_loss: 0.6931 - val_accuracy: 0.5013\n",
      "Epoch 7/20\n",
      "141/141 [==============================] - 2s 11ms/step - loss: 0.6937 - accuracy: 0.4951 - val_loss: 0.6935 - val_accuracy: 0.4880\n",
      "Epoch 8/20\n",
      "141/141 [==============================] - 2s 11ms/step - loss: 0.6934 - accuracy: 0.4940 - val_loss: 0.6939 - val_accuracy: 0.4915\n",
      "Epoch 9/20\n",
      "141/141 [==============================] - 2s 11ms/step - loss: 0.6935 - accuracy: 0.5065 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 10/20\n",
      "141/141 [==============================] - 2s 12ms/step - loss: 0.6935 - accuracy: 0.4893 - val_loss: 0.6933 - val_accuracy: 0.5094\n",
      "Epoch 11/20\n",
      "141/141 [==============================] - 2s 11ms/step - loss: 0.6937 - accuracy: 0.4933 - val_loss: 0.6935 - val_accuracy: 0.4915\n",
      "Epoch 12/20\n",
      "141/141 [==============================] - 2s 11ms/step - loss: 0.6935 - accuracy: 0.4891 - val_loss: 0.6936 - val_accuracy: 0.4897\n",
      "Epoch 13/20\n",
      "141/141 [==============================] - 2s 11ms/step - loss: 0.6934 - accuracy: 0.4971 - val_loss: 0.6934 - val_accuracy: 0.5067\n",
      "Epoch 14/20\n",
      "141/141 [==============================] - 2s 11ms/step - loss: 0.6934 - accuracy: 0.4955 - val_loss: 0.6936 - val_accuracy: 0.4915\n",
      "Epoch 15/20\n",
      "141/141 [==============================] - 2s 11ms/step - loss: 0.6933 - accuracy: 0.5022 - val_loss: 0.6932 - val_accuracy: 0.4888\n",
      "Epoch 16/20\n",
      "141/141 [==============================] - 2s 11ms/step - loss: 0.6936 - accuracy: 0.4926 - val_loss: 0.6933 - val_accuracy: 0.5156\n",
      "Epoch 17/20\n",
      "141/141 [==============================] - 2s 11ms/step - loss: 0.6933 - accuracy: 0.5009 - val_loss: 0.6934 - val_accuracy: 0.4880\n",
      "Epoch 18/20\n",
      "141/141 [==============================] - 2s 12ms/step - loss: 0.6933 - accuracy: 0.4980 - val_loss: 0.6933 - val_accuracy: 0.5112\n",
      "Epoch 19/20\n",
      "141/141 [==============================] - 2s 11ms/step - loss: 0.6934 - accuracy: 0.5004 - val_loss: 0.6933 - val_accuracy: 0.5174\n",
      "Epoch 20/20\n",
      "141/141 [==============================] - 2s 13ms/step - loss: 0.6933 - accuracy: 0.5004 - val_loss: 0.6936 - val_accuracy: 0.4915\n",
      "44/44 [==============================] - 1s 4ms/step\n",
      "Accuracy: 0.4989293361884368\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from gensim.models import Word2Vec\n",
    "from datasets import load_dataset\n",
    "\n",
    "pic_dataset = load_dataset('PiC/phrase_similarity', 'PS-hard')\n",
    "\n",
    "pic_df = pd.DataFrame(pic_dataset['train'])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "pic_df['label'] = label_encoder.fit_transform(pic_df['label'])\n",
    "\n",
    "corpus = pic_df['sentence1'].tolist() + pic_df['sentence2'].tolist()\n",
    "fasttext_model = FastText(sentences=[sentence.split() for sentence in corpus], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "def get_sentence_vector(sentence, model):\n",
    "    words = sentence.split()\n",
    "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if vectors:\n",
    "        return sum(vectors) / len(vectors)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "pic_df['vector_sentence1'] = pic_df['sentence1'].apply(lambda x: get_sentence_vector(x, fasttext_model))\n",
    "pic_df['vector_sentence2'] = pic_df['sentence2'].apply(lambda x: get_sentence_vector(x, fasttext_model))\n",
    "\n",
    "pic_df = pic_df.dropna(subset=['vector_sentence1', 'vector_sentence2'])\n",
    "\n",
    "\n",
    "X = np.array(list(zip(pic_df['vector_sentence1'], pic_df['vector_sentence2'])))\n",
    "y = pic_df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(2, 100)))  # Adjust input_shape based on your sentence vector dimensions\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2351e8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
